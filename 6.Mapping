import numpy as np
import pandas as pd
import itertools
import joblib
from sklearn.neighbors import NearestNeighbors

def map_target_batch(base_models, meta_model, X_scaler, y_scaler, batch_size=10000):
    feature_values = [
        np.arange(0, 70, 1),  # 'Lignin (wt%)'
        np.arange(0.6, 1.4 + 0.2, 0.05),  # 'Ratio'
        [250, 650, 1000],  # 'Co-polyol type (PTHF)'
        np.arange(0, 20, 0.4),  # 'Isocyanate (mmol NCO)'
        [0, 1],  # 'Isocyanate type'
        np.arange(0, 2, 0.5),  # 'Tin(II) octoate'
        np.linspace(0, 472, 2)  # 'Swelling ratio (%)'
    ]
    
    columns = ['Lignin (wt%)', 'Ratio', 'Co-polyol type (PTHF)', 
               'Isocyanate (mmol NCO)', 'Isocyanate type', 'Tin(II) octoate', 
               'Swelling ratio (%)', 'Predicted_Tg']
    
    results = pd.DataFrame(columns=columns)
    
    total_combinations = np.prod([len(fv) for fv in feature_values])
    print(f"Total combinations to process: {total_combinations}")
    
    for i, combo in enumerate(itertools.product(*feature_values)):
        if i % batch_size == 0:
            print(f"Processing batch {i // batch_size + 1}")
            batch = []
        
        input_data = pd.DataFrame([combo], columns=columns[:-1])
        input_data_scaled = X_scaler.transform(input_data)
        
        base_predictions = np.column_stack([model.predict(input_data_scaled) for model in base_models])
        prediction_scaled = meta_model.predict(base_predictions)
        prediction = y_scaler.inverse_transform(prediction_scaled.reshape(-1, 1))[0][0]
        
        batch.append((*combo, prediction))
        
        if (i + 1) % batch_size == 0 or i == total_combinations - 1:
            batch_df = pd.DataFrame(batch, columns=columns)
            results = pd.concat([results, batch_df], ignore_index=True)
            batch = []
    
    return results

def find_closest_inputs(mapped_results, target_tgs, n_neighbors=1):
    # Create a NearestNeighbors object
    nn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')
    
    # Fit the NearestNeighbors object to the Predicted_Tg values
    nn.fit(mapped_results[['Predicted_Tg']])
    
    # Find the n_neighbors closest Predicted_Tg values for each target Tg
    distances, indices = nn.kneighbors(np.array(target_tgs).reshape(-1, 1))
    
    # Create a list to store the results
    closest_inputs = []
    
    # For each target Tg, get the corresponding input parameters
    for i, target_tg in enumerate(target_tgs):
        for j in range(n_neighbors):
            closest_row = mapped_results.iloc[indices[i][j]]
            closest_inputs.append({
                'Target_Tg': target_tg,
                'Predicted_Tg': closest_row['Predicted_Tg'],
                'Lignin (wt%)': closest_row['Lignin (wt%)'],
                'Ratio': closest_row['Ratio'],
                'Co-polyol type (PTHF)': closest_row['Co-polyol type (PTHF)'],
                'Isocyanate (mmol NCO)': closest_row['Isocyanate (mmol NCO)'],
                'Isocyanate type': closest_row['Isocyanate type'],
                'Tin(II) octoate': closest_row['Tin(II) octoate'],
                'Swelling ratio (%)': closest_row['Swelling ratio (%)']
            })
    
    return pd.DataFrame(closest_inputs)

# Load the saved models and scalers
run_number = 1  # Change this to the appropriate run number
base_models = joblib.load(f'base_models_run_{run_number}.joblib')
meta_model = joblib.load(f'meta_model_run_{run_number}.joblib')
X_scaler = joblib.load(f'X_scaler_run_{run_number}.joblib')
y_scaler = joblib.load(f'y_scaler_run_{run_number}.joblib')

# Map the target using all feature variations
mapped_results = map_target_batch(base_models, meta_model, X_scaler, y_scaler)

# Save the mapped results
mapped_results.to_csv('mapped_results_tg.csv', index=False)
print("Mapped results saved to 'mapped_results_tg.csv'")

# Define a list of target Tg values
# target_tgs = [20, 30, 40, 50, 60]  # Add your desired target Tg values here
target_tgs = list(range(-20,121))  # Add your desired target Tg values here

# Find the closest input parameters for the target Tg values
closest_inputs = find_closest_inputs(mapped_results, target_tgs, n_neighbors=3)

# Save the closest inputs to a CSV file
closest_inputs.to_csv('closest_inputs.csv', index=False)
print("Closest inputs for target Tgs saved to 'closest_inputs_for_target_tgs.csv'")

# Print the results
print("\nClosest inputs for target Tgs:")
print(closest_inputs)

# Additional analysis (optional)
print("\nSummary of mapped results:")
print(mapped_results.describe())

# Find the combination with the highest predicted Tg
max_tg = mapped_results.loc[mapped_results['Predicted_Tg'].idxmax()]
print("\nCombination with highest predicted Tg:")
print(max_tg)

# Calculate the median of the Predicted_Tg
median_tg = mapped_results['Predicted_Tg'].median()
print("\nMedian of Predicted_Tg:")
print(median_tg)

# Find the combination with the lowest predicted Tg
min_tg = mapped_results.loc[mapped_results['Predicted_Tg'].idxmin()]
print("\nCombination with lowest predicted Tg:")
print(min_tg)

# Analyze the impact of each feature on the Tg
for feature in ['Lignin (wt%)', 'Ratio', 'Co-polyol type (PTHF)', 
                'Isocyanate (mmol NCO)', 'Isocyanate type', 
                'Tin(II) octoate', 'Swelling ratio (%)']:
    correlation = mapped_results[feature].corr(mapped_results['Predicted_Tg'])
    print(f"\nCorrelation between {feature} and Predicted_Tg: {correlation}")

# Analyze the impact of Isocyanate type on the Tg
isocyanate_impact = mapped_results.groupby('Isocyanate type')['Predicted_Tg'].mean()
print("\nAverage Predicted_Tg for each Isocyanate type:")
print(isocyanate_impact)
